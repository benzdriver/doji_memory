# æ‰¹é‡å¤„ç†åŠŸèƒ½å®Œæ•´æŒ‡å—

> **Version**: 1.0  
> **Last Updated**: 2024-12-19  
> **Author**: Weaviate Memory System Team

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [ä½¿ç”¨åœºæ™¯](#ä½¿ç”¨åœºæ™¯)
- [APIå‚è€ƒ](#apiå‚è€ƒ)
- [å®è·µæ¡ˆä¾‹](#å®è·µæ¡ˆä¾‹)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ğŸ“– æ¦‚è¿°

æ‰¹é‡å¤„ç†åŠŸèƒ½æ˜¯Weaviate Memory Systemçš„æ ¸å¿ƒç‰¹æ€§ä¹‹ä¸€ï¼Œæ—¨åœ¨æ˜¾è‘—æé«˜å¤§è§„æ¨¡æ–‡æœ¬å¤„ç†çš„æ•ˆç‡ã€‚é€šè¿‡æ™ºèƒ½ç¼“å­˜å’ŒAPIè°ƒç”¨ä¼˜åŒ–ï¼Œæ‰¹é‡å¤„ç†å¯ä»¥å°†æ€§èƒ½æå‡60%ä»¥ä¸Šã€‚

### ğŸ¯ ä¸»è¦ä¼˜åŠ¿

- **ğŸš€ æ€§èƒ½æå‡**: 60-80%çš„å¤„ç†é€Ÿåº¦æå‡
- **ğŸ’° æˆæœ¬ä¼˜åŒ–**: å‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼Œé™ä½æˆæœ¬
- **ğŸ§  æ™ºèƒ½ç¼“å­˜**: è‡ªåŠ¨æ£€æµ‹ç¼“å­˜å‘½ä¸­ï¼Œé¿å…é‡å¤è®¡ç®—
- **ğŸ”§ æ˜“äºä½¿ç”¨**: ä¸å•æ–‡æœ¬APIä¿æŒä¸€è‡´çš„ä½¿ç”¨ä½“éªŒ

---

## ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µ

### æ‰¹é‡Embeddingç”Ÿæˆ

```python
# å•æ–‡æœ¬å¤„ç†ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
vector1 = embed_text("æ–‡æœ¬1")
vector2 = embed_text("æ–‡æœ¬2") 
vector3 = embed_text("æ–‡æœ¬3")

# æ‰¹é‡å¤„ç†ï¼ˆä¼˜åŒ–æ–¹å¼ï¼‰
vectors = embed_texts_batch(["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"])
```

### æ‰¹é‡å†…å­˜å†™å…¥

```python
# å•ä¸ªå†™å…¥ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
uuid1 = write_memory(content="å†…å®¹1", project="proj", repo="repo", agent="agent", tags=["tag1"])
uuid2 = write_memory(content="å†…å®¹2", project="proj", repo="repo", agent="agent", tags=["tag2"])

# æ‰¹é‡å†™å…¥ï¼ˆä¼˜åŒ–æ–¹å¼ï¼‰
memories = [
    {"content": "å†…å®¹1", "project": "proj", "repo": "repo", "agent": "agent", "tags": ["tag1"]},
    {"content": "å†…å®¹2", "project": "proj", "repo": "repo", "agent": "agent", "tags": ["tag2"]}
]
uuids = write_memories_batch(memories)
```

### æ™ºèƒ½ç¼“å­˜æœºåˆ¶

```
è¾“å…¥: ["å·²ç¼“å­˜æ–‡æœ¬", "æ–°æ–‡æœ¬", "å·²ç¼“å­˜æ–‡æœ¬2"]
      â†“
ç¼“å­˜æ£€æŸ¥: [âœ… å‘½ä¸­, âŒ æœªå‘½ä¸­, âœ… å‘½ä¸­]
      â†“
APIè°ƒç”¨: åªå¤„ç† "æ–°æ–‡æœ¬"
      â†“
ç»“æœç»„è£…: [ç¼“å­˜å€¼, APIç»“æœ, ç¼“å­˜å€¼]
```

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: çŸ¥è¯†åº“æ‰¹é‡å¯¼å…¥

**ä¸šåŠ¡éœ€æ±‚**: å°†å¤§é‡æ–‡æ¡£æ‰¹é‡å¯¼å…¥çŸ¥è¯†åº“ç³»ç»Ÿ

```python
def import_knowledge_base(documents):
    """æ‰¹é‡å¯¼å…¥çŸ¥è¯†åº“æ–‡æ¡£"""
    
    # é…ç½®æ‰¹æ¬¡å¤§å°
    BATCH_SIZE = 50
    total_imported = 0
    
    for i in range(0, len(documents), BATCH_SIZE):
        batch = documents[i:i + BATCH_SIZE]
        
        # å‡†å¤‡æ‰¹é‡å†…å­˜æ•°æ®
        memories = []
        for doc in batch:
            memories.append({
                "content": doc.content,
                "project": "knowledge-base",
                "repo": doc.source_repo,
                "agent": "import-bot",
                "tags": ["knowledge", doc.category, doc.language],
                "source": "document-import"
            })
        
        # æ‰¹é‡å†™å…¥
        try:
            uuids = write_memories_batch(memories)
            total_imported += len(uuids)
            
            print(f"âœ… æ‰¹æ¬¡ {i//BATCH_SIZE + 1}: æˆåŠŸå¯¼å…¥ {len(uuids)} ä¸ªæ–‡æ¡£")
            print(f"ğŸ“Š æ€»è¿›åº¦: {total_imported}/{len(documents)} ({total_imported/len(documents)*100:.1f}%)")
            
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {i//BATCH_SIZE + 1} å¯¼å…¥å¤±è´¥: {e}")
            continue
    
    return total_imported
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
# å‡†å¤‡æ–‡æ¡£æ•°æ®
documents = [
    Document(content="Pythonç¼–ç¨‹æŒ‡å—...", source_repo="docs", category="programming", language="zh"),
    Document(content="APIæ¥å£æ–‡æ¡£...", source_repo="api", category="reference", language="zh"),
    # ... æ›´å¤šæ–‡æ¡£
]

# æ‰§è¡Œæ‰¹é‡å¯¼å…¥
imported_count = import_knowledge_base(documents)
print(f"ğŸ‰ æ€»å…±æˆåŠŸå¯¼å…¥ {imported_count} ä¸ªæ–‡æ¡£")
```

### åœºæ™¯2: å®æ—¶å¯¹è¯è®°å½•å¤„ç†

**ä¸šåŠ¡éœ€æ±‚**: å¤„ç†å®¢æœç³»ç»Ÿä¸­ç§¯ç´¯çš„å¯¹è¯è®°å½•

```python
class ConversationProcessor:
    """å¯¹è¯è®°å½•æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self):
        self.batch_size = 30
        self.pending_conversations = []
    
    def add_conversation(self, conversation):
        """æ·»åŠ å•ä¸ªå¯¹è¯åˆ°å¾…å¤„ç†é˜Ÿåˆ—"""
        self.pending_conversations.append(conversation)
        
        # è¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶è‡ªåŠ¨å¤„ç†
        if len(self.pending_conversations) >= self.batch_size:
            self.process_batch()
    
    def process_batch(self):
        """å¤„ç†å½“å‰æ‰¹æ¬¡çš„å¯¹è¯"""
        if not self.pending_conversations:
            return
        
        memories = []
        for conv in self.pending_conversations:
            memories.append({
                "content": conv.content,
                "project": "customer-support",
                "repo": f"conversations-{conv.language}",
                "agent": conv.agent_id,
                "tags": ["customer-support", conv.category, conv.language, conv.sentiment],
                "source": "live-chat"
            })
        
        try:
            uuids = write_memories_batch(memories)
            print(f"âœ… æˆåŠŸå¤„ç† {len(uuids)} æ¡å¯¹è¯è®°å½•")
            
            # æ¸…ç©ºå¾…å¤„ç†é˜Ÿåˆ—
            self.pending_conversations.clear()
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
    
    def flush(self):
        """å¼ºåˆ¶å¤„ç†å‰©ä½™çš„å¯¹è¯"""
        if self.pending_conversations:
            self.process_batch()
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
processor = ConversationProcessor()

# æ¨¡æ‹Ÿå®æ—¶å¯¹è¯æµ
for conversation in real_time_conversations:
    processor.add_conversation(conversation)

# ç¡®ä¿æ‰€æœ‰å¯¹è¯éƒ½è¢«å¤„ç†
processor.flush()
```

### åœºæ™¯3: ä»£ç ä»“åº“åˆ†æ

**ä¸šåŠ¡éœ€æ±‚**: åˆ†æGitä»“åº“çš„æäº¤è®°å½•å’Œä»£ç æ³¨é‡Š

```python
def analyze_repository_commits(repo_path, since_date=None):
    """åˆ†æä»“åº“æäº¤è®°å½•"""
    
    # è·å–æäº¤ä¿¡æ¯ï¼ˆç¤ºä¾‹ï¼Œå®é™…éœ€è¦ä½¿ç”¨gitåº“ï¼‰
    commits = get_git_commits(repo_path, since=since_date)
    
    # æŒ‰æ—¶é—´æ®µåˆ†ç»„æ‰¹é‡å¤„ç†
    commit_batches = group_commits_by_timeframe(commits, days=7)
    
    all_results = []
    
    for week, week_commits in commit_batches.items():
        print(f"ğŸ“… å¤„ç† {week} çš„æäº¤è®°å½• ({len(week_commits)} ä¸ªæäº¤)")
        
        # å‡†å¤‡æ‰¹é‡æ•°æ®
        commit_memories = []
        for commit in week_commits:
            # ç»“åˆæäº¤ä¿¡æ¯å’Œä»£ç å·®å¼‚
            content = f"æäº¤: {commit.message}\næ–‡ä»¶å˜æ›´: {', '.join(commit.changed_files)}\nä»£ç è¡Œæ•°: +{commit.additions}/-{commit.deletions}"
            
            commit_memories.append({
                "content": content,
                "project": f"repo-{repo_path.name}",
                "repo": commit.branch,
                "agent": commit.author,
                "tags": ["git-commit", commit.type, f"week-{week}"],
                "source": "git-analysis"
            })
        
        # æ‰¹é‡å¤„ç†æäº¤è®°å½•
        try:
            uuids = write_memories_batch(commit_memories)
            all_results.extend(uuids)
            
            print(f"âœ… æˆåŠŸåˆ†æ {len(uuids)} ä¸ªæäº¤")
            
        except Exception as e:
            print(f"âŒ å¤„ç† {week} æäº¤å¤±è´¥: {e}")
            continue
    
    return all_results
```

### åœºæ™¯4: å¤šè¯­è¨€å†…å®¹å¤„ç†

**ä¸šåŠ¡éœ€æ±‚**: å¤„ç†å¤šè¯­è¨€å®¢æœæ•°æ®

```python
class MultilingualProcessor:
    """å¤šè¯­è¨€å†…å®¹æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self):
        self.language_queues = {}
        self.batch_sizes = {
            'zh': 40,  # ä¸­æ–‡æ‰¹æ¬¡å¤§å°
            'en': 50,  # è‹±æ–‡æ‰¹æ¬¡å¤§å°  
            'ja': 30,  # æ—¥æ–‡æ‰¹æ¬¡å¤§å°
            'default': 35
        }
    
    def add_content(self, content, language, metadata):
        """æ·»åŠ å†…å®¹åˆ°å¯¹åº”è¯­è¨€é˜Ÿåˆ—"""
        if language not in self.language_queues:
            self.language_queues[language] = []
        
        self.language_queues[language].append({
            'content': content,
            'language': language,
            'metadata': metadata
        })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†è¯¥è¯­è¨€çš„æ‰¹æ¬¡
        batch_size = self.batch_sizes.get(language, self.batch_sizes['default'])
        if len(self.language_queues[language]) >= batch_size:
            self.process_language_batch(language)
    
    def process_language_batch(self, language):
        """å¤„ç†ç‰¹å®šè¯­è¨€çš„æ‰¹æ¬¡"""
        if language not in self.language_queues or not self.language_queues[language]:
            return
        
        items = self.language_queues[language]
        
        # ä¸ºè¯¥è¯­è¨€ä¼˜åŒ–çš„å¤„ç†é€»è¾‘
        memories = []
        for item in items:
            memories.append({
                "content": item['content'],
                "project": "multilingual-support",
                "repo": f"content-{language}",
                "agent": item['metadata'].get('agent', 'system'),
                "tags": ["multilingual", language, item['metadata'].get('category', 'general')],
                "source": f"multilingual-{language}"
            })
        
        try:
            uuids = write_memories_batch(memories)
            print(f"ğŸŒ {language.upper()}: æˆåŠŸå¤„ç† {len(uuids)} æ¡å†…å®¹")
            
            # æ¸…ç©ºè¯¥è¯­è¨€é˜Ÿåˆ—
            self.language_queues[language].clear()
            
        except Exception as e:
            print(f"âŒ {language.upper()} æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
    
    def process_all_remaining(self):
        """å¤„ç†æ‰€æœ‰å‰©ä½™çš„å†…å®¹"""
        for language in list(self.language_queues.keys()):
            if self.language_queues[language]:
                self.process_language_batch(language)
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
processor = MultilingualProcessor()

# æ¨¡æ‹Ÿå¤šè¯­è¨€å†…å®¹æµ
multilingual_contents = [
    ("ä½ å¥½ï¼Œæˆ‘éœ€è¦å¸®åŠ©", "zh", {"agent": "agent-01", "category": "help"}),
    ("Hello, I have a question", "en", {"agent": "agent-02", "category": "question"}),
    ("ã“ã‚“ã«ã¡ã¯ã€è³ªå•ãŒã‚ã‚Šã¾ã™", "ja", {"agent": "agent-03", "category": "question"}),
]

for content, lang, meta in multilingual_contents:
    processor.add_content(content, lang, meta)

# å¤„ç†æ‰€æœ‰å‰©ä½™å†…å®¹
processor.process_all_remaining()
```

---

## ğŸ”§ APIå‚è€ƒ

### embed_texts_batch()

æ‰¹é‡ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ã€‚

**ç­¾å**:
```python
def embed_texts_batch(
    texts: List[str], 
    metadata: Optional[List[Dict]] = None
) -> List[List[float]]
```

**å‚æ•°**:
- `texts`: è¦å¤„ç†çš„æ–‡æœ¬åˆ—è¡¨
- `metadata`: å¯é€‰çš„å…ƒæ•°æ®åˆ—è¡¨ï¼Œé•¿åº¦å¿…é¡»ä¸textsç›¸åŒ

**è¿”å›å€¼**: 
- `List[List[float]]`: åµŒå…¥å‘é‡åˆ—è¡¨ï¼Œé¡ºåºä¸è¾“å…¥æ–‡æœ¬å¯¹åº”

**å¼‚å¸¸**:
- `ValueError`: è¾“å…¥å‚æ•°æ— æ•ˆ
- `OpenAIError`: OpenAI APIè°ƒç”¨å¤±è´¥

**ç¤ºä¾‹**:
```python
# åŸºæœ¬ä½¿ç”¨
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
vectors = embed_texts_batch(texts)

# å¸¦å…ƒæ•°æ®
metadata = [{"type": "title"}, {"type": "content"}, {"type": "summary"}]
vectors = embed_texts_batch(texts, metadata=metadata)
```

### write_memories_batch()

æ‰¹é‡å†™å…¥å†…å­˜è®°å½•åˆ°Weaviateæ•°æ®åº“ã€‚

**ç­¾å**:
```python
def write_memories_batch(
    memories: List[Dict[str, Any]]
) -> List[str]
```

**å‚æ•°**:
- `memories`: å†…å­˜è®°å½•å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸å¿…é¡»åŒ…å«:
  - `content` (str): å†…å®¹æ–‡æœ¬
  - `project` (str): é¡¹ç›®æ ‡è¯†
  - `repo` (str): ä»“åº“åç§°
  - `agent` (str): ä»£ç†æ ‡è¯†
  - `tags` (List[str]): æ ‡ç­¾åˆ—è¡¨
  - `source` (str, å¯é€‰): æ¥æºï¼Œé»˜è®¤"agent"

**è¿”å›å€¼**:
- `List[str]`: åˆ›å»ºçš„è®°å½•UUIDåˆ—è¡¨

**å¼‚å¸¸**:
- `ValueError`: è¾“å…¥æ•°æ®éªŒè¯å¤±è´¥
- `Exception`: Weaviateå†™å…¥å¤±è´¥

**ç¤ºä¾‹**:
```python
memories = [
    {
        "content": "å®ç°äº†æ–°åŠŸèƒ½",
        "project": "my-project",
        "repo": "main",
        "agent": "developer", 
        "tags": ["feature", "implementation"],
        "source": "development"
    },
    {
        "content": "ä¿®å¤äº†bug",
        "project": "my-project",
        "repo": "main",
        "agent": "developer",
        "tags": ["bugfix"]
        # source å°†é»˜è®¤ä¸º "agent"
    }
]

uuids = write_memories_batch(memories)
```

### EmbeddingRouter.get_embeddings_batch()

ä½çº§åˆ«çš„æ‰¹é‡åµŒå…¥æ¥å£ï¼Œæä¾›æ›´å¤šæ§åˆ¶é€‰é¡¹ã€‚

**ç­¾å**:
```python
def get_embeddings_batch(
    self,
    texts: List[str],
    use_cache: bool = True,
    metadata: Optional[List[Dict]] = None
) -> List[List[float]]
```

**å‚æ•°**:
- `texts`: æ–‡æœ¬åˆ—è¡¨
- `use_cache`: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤True
- `metadata`: å…ƒæ•°æ®åˆ—è¡¨

**ç¤ºä¾‹**:
```python
from vector.embedding_router import EmbeddingRouter

router = EmbeddingRouter()

# ç¦ç”¨ç¼“å­˜çš„æ‰¹é‡å¤„ç†
vectors = router.get_embeddings_batch(
    texts=["æ–‡æœ¬1", "æ–‡æœ¬2"], 
    use_cache=False
)

# å¸¦å…ƒæ•°æ®å’Œç¼“å­˜
metadata = [{"source": "doc1"}, {"source": "doc2"}]
vectors = router.get_embeddings_batch(
    texts=["æ–‡æœ¬1", "æ–‡æœ¬2"],
    use_cache=True,
    metadata=metadata
)
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### æ‰¹æ¬¡å¤§å°é€‰æ‹©

| å†…å®¹ç±»å‹ | æ¨èæ‰¹æ¬¡å¤§å° | è¯´æ˜ |
|---------|-------------|------|
| çŸ­æ–‡æœ¬ (< 100å­—ç¬¦) | 40-60 | ç¤¾äº¤åª’ä½“ã€èŠå¤©è®°å½• |
| ä¸­ç­‰æ–‡æœ¬ (100-500å­—ç¬¦) | 30-50 | æ–°é—»æ ‡é¢˜ã€äº§å“æè¿° |
| é•¿æ–‡æœ¬ (500-2000å­—ç¬¦) | 20-40 | æ–‡ç« æ®µè½ã€æ–‡æ¡£ç« èŠ‚ |
| è¶…é•¿æ–‡æœ¬ (> 2000å­—ç¬¦) | 10-25 | å®Œæ•´æ–‡æ¡£ã€æŠ¥å‘Š |

### ç¼“å­˜ä¼˜åŒ–ç­–ç•¥

```python
# ç­–ç•¥1: é¢„çƒ­ç¼“å­˜
def warmup_cache(common_texts):
    """é¢„çƒ­å¸¸ç”¨æ–‡æœ¬çš„ç¼“å­˜"""
    router = EmbeddingRouter()
    
    # æ‰¹é‡ç”Ÿæˆå¸¸ç”¨æ–‡æœ¬çš„embedding
    router.get_embeddings_batch(common_texts)
    
    print(f"âœ… ç¼“å­˜é¢„çƒ­å®Œæˆï¼Œé¢„çƒ­äº† {len(common_texts)} ä¸ªå¸¸ç”¨æ–‡æœ¬")

# ç­–ç•¥2: ç¼“å­˜åˆ†æ
def analyze_cache_performance():
    """åˆ†æç¼“å­˜æ€§èƒ½"""
    router = EmbeddingRouter()
    cache_info = router.get_cache_info()
    
    print(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
    print(f"   ç¼“å­˜æ¡ç›®æ•°: {cache_info['num_entries']}")
    print(f"   ç¼“å­˜å¤§å°: {cache_info['total_size_bytes'] / 1024 / 1024:.2f} MB")
    
    # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡ï¼ˆéœ€è¦åœ¨åº”ç”¨å±‚å®ç°ï¼‰
    return cache_info
```

### å†…å­˜ç®¡ç†

```python
import gc
from typing import Iterator

def process_large_dataset_in_chunks(
    dataset: List[str], 
    chunk_size: int = 50
) -> Iterator[List[str]]:
    """å¤§æ•°æ®é›†åˆ†å—å¤„ç†ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
    
    for i in range(0, len(dataset), chunk_size):
        chunk = dataset[i:i + chunk_size]
        
        # å¤„ç†å½“å‰å—
        try:
            uuids = write_memories_batch(prepare_memories(chunk))
            yield uuids
            
        except Exception as e:
            print(f"âŒ å— {i//chunk_size + 1} å¤„ç†å¤±è´¥: {e}")
            continue
        
        # æ‰‹åŠ¨è§¦å‘åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜
        if i % (chunk_size * 10) == 0:
            gc.collect()

# ä½¿ç”¨ç¤ºä¾‹
total_processed = 0
for batch_uuids in process_large_dataset_in_chunks(large_dataset):
    total_processed += len(batch_uuids)
    print(f"ğŸ“ˆ å·²å¤„ç† {total_processed} æ¡è®°å½•")
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

```python
import time
import random
from typing import Optional

class BatchProcessor:
    """å¸¦é‡è¯•æœºåˆ¶çš„æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
    
    def process_with_retry(self, memories: List[Dict]) -> Optional[List[str]]:
        """å¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•æœºåˆ¶"""
        
        for attempt in range(self.max_retries + 1):
            try:
                return write_memories_batch(memories)
                
            except Exception as e:
                if attempt == self.max_retries:
                    print(f"âŒ æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯• {self.max_retries} æ¬¡: {e}")
                    return None
                
                # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
                delay = self.base_delay * (2 ** attempt) + random.uniform(0, 1)
                print(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œ{delay:.1f}ç§’åé‡è¯•: {e}")
                time.sleep(delay)
        
        return None
```

### 2. è¿›åº¦è·Ÿè¸ªå’Œç›‘æ§

```python
from tqdm import tqdm
import time

class ProgressTracker:
    """æ‰¹é‡å¤„ç†è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self, total_items: int, batch_size: int):
        self.total_items = total_items
        self.batch_size = batch_size
        self.processed_items = 0
        self.start_time = time.time()
        
        # åˆ›å»ºè¿›åº¦æ¡
        self.pbar = tqdm(
            total=total_items,
            desc="æ‰¹é‡å¤„ç†",
            unit="items",
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
        )
    
    def update_progress(self, batch_size: int):
        """æ›´æ–°è¿›åº¦"""
        self.processed_items += batch_size
        self.pbar.update(batch_size)
        
        # è®¡ç®—ETAå’Œé€Ÿåº¦
        elapsed = time.time() - self.start_time
        if elapsed > 0:
            rate = self.processed_items / elapsed
            remaining = (self.total_items - self.processed_items) / rate if rate > 0 else 0
            
            self.pbar.set_postfix({
                'é€Ÿåº¦': f'{rate:.1f} items/s',
                'é¢„è®¡å‰©ä½™': f'{remaining:.0f}s'
            })
    
    def close(self):
        """å…³é—­è¿›åº¦æ¡"""
        self.pbar.close()
        
        total_time = time.time() - self.start_time
        avg_rate = self.processed_items / total_time if total_time > 0 else 0
        
        print(f"ğŸ‰ å¤„ç†å®Œæˆ!")
        print(f"   æ€»è®¡: {self.processed_items} é¡¹")
        print(f"   è€—æ—¶: {total_time:.1f} ç§’")
        print(f"   å¹³å‡é€Ÿåº¦: {avg_rate:.1f} items/s")

# ä½¿ç”¨ç¤ºä¾‹
def process_with_progress(all_memories: List[Dict], batch_size: int = 50):
    """å¸¦è¿›åº¦è·Ÿè¸ªçš„æ‰¹é‡å¤„ç†"""
    
    tracker = ProgressTracker(len(all_memories), batch_size)
    processor = BatchProcessor()
    
    try:
        for i in range(0, len(all_memories), batch_size):
            batch = all_memories[i:i + batch_size]
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            result = processor.process_with_retry(batch)
            
            if result:
                tracker.update_progress(len(batch))
            else:
                print(f"âš ï¸ è·³è¿‡å¤±è´¥çš„æ‰¹æ¬¡ {i//batch_size + 1}")
    
    finally:
        tracker.close()
```

### 3. é…ç½®ç®¡ç†

```python
from dataclasses import dataclass
from typing import Dict, Any
import json

@dataclass
class BatchConfig:
    """æ‰¹é‡å¤„ç†é…ç½®"""
    
    # æ‰¹æ¬¡å¤§å°é…ç½®
    default_batch_size: int = 50
    text_type_batch_sizes: Dict[str, int] = None
    
    # é‡è¯•é…ç½®
    max_retries: int = 3
    base_retry_delay: float = 1.0
    
    # ç¼“å­˜é…ç½®
    enable_cache: bool = True
    cache_dir: str = ".cache/embeddings"
    
    # æ€§èƒ½é…ç½®
    enable_progress_bar: bool = True
    gc_frequency: int = 10  # æ¯å¤„ç†å¤šå°‘ä¸ªæ‰¹æ¬¡è§¦å‘åƒåœ¾å›æ”¶
    
    # é”™è¯¯å¤„ç†
    continue_on_error: bool = True
    log_errors: bool = True
    
    def __post_init__(self):
        if self.text_type_batch_sizes is None:
            self.text_type_batch_sizes = {
                'short': 60,     # < 100 å­—ç¬¦
                'medium': 40,    # 100-500 å­—ç¬¦  
                'long': 25,      # 500-2000 å­—ç¬¦
                'very_long': 15  # > 2000 å­—ç¬¦
            }
    
    @classmethod
    def from_file(cls, config_path: str) -> 'BatchConfig':
        """ä»é…ç½®æ–‡ä»¶åŠ è½½"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        return cls(**config_data)
    
    def get_batch_size_for_text(self, text: str) -> int:
        """æ ¹æ®æ–‡æœ¬é•¿åº¦è·å–æœ€é€‚åˆçš„æ‰¹æ¬¡å¤§å°"""
        text_len = len(text)
        
        if text_len < 100:
            return self.text_type_batch_sizes['short']
        elif text_len < 500:
            return self.text_type_batch_sizes['medium']
        elif text_len < 2000:
            return self.text_type_batch_sizes['long']
        else:
            return self.text_type_batch_sizes['very_long']
```

### 4. ç›‘æ§å’Œåº¦é‡

```python
import time
from collections import defaultdict
from typing import Dict, List

class BatchMetrics:
    """æ‰¹é‡å¤„ç†åº¦é‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """é‡ç½®æ‰€æœ‰åº¦é‡"""
        self.start_time = time.time()
        self.batch_count = 0
        self.total_items = 0
        self.successful_items = 0
        self.failed_items = 0
        self.cache_hits = 0
        self.cache_misses = 0
        self.processing_times = []
        self.error_counts = defaultdict(int)
    
    def record_batch(
        self, 
        batch_size: int, 
        processing_time: float, 
        success: bool,
        cache_hit_count: int = 0
    ):
        """è®°å½•æ‰¹æ¬¡å¤„ç†ç»“æœ"""
        self.batch_count += 1
        self.total_items += batch_size
        self.processing_times.append(processing_time)
        
        if success:
            self.successful_items += batch_size
        else:
            self.failed_items += batch_size
        
        self.cache_hits += cache_hit_count
        self.cache_misses += (batch_size - cache_hit_count)
    
    def record_error(self, error_type: str):
        """è®°å½•é”™è¯¯ç±»å‹"""
        self.error_counts[error_type] += 1
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–åº¦é‡æ‘˜è¦"""
        total_time = time.time() - self.start_time
        avg_processing_time = sum(self.processing_times) / len(self.processing_times) if self.processing_times else 0
        
        return {
            'æ€»å¤„ç†æ—¶é—´': f'{total_time:.2f}s',
            'å¹³å‡æ‰¹æ¬¡å¤„ç†æ—¶é—´': f'{avg_processing_time:.2f}s',
            'æ€»æ‰¹æ¬¡æ•°': self.batch_count,
            'æ€»é¡¹ç›®æ•°': self.total_items,
            'æˆåŠŸé¡¹ç›®æ•°': self.successful_items,
            'å¤±è´¥é¡¹ç›®æ•°': self.failed_items,
            'æˆåŠŸç‡': f'{(self.successful_items / self.total_items * 100):.1f}%' if self.total_items > 0 else '0%',
            'ç¼“å­˜å‘½ä¸­æ•°': self.cache_hits,
            'ç¼“å­˜æœªå‘½ä¸­æ•°': self.cache_misses,
            'ç¼“å­˜å‘½ä¸­ç‡': f'{(self.cache_hits / (self.cache_hits + self.cache_misses) * 100):.1f}%' if (self.cache_hits + self.cache_misses) > 0 else '0%',
            'å¤„ç†é€Ÿåº¦': f'{self.total_items / total_time:.1f} items/s' if total_time > 0 else '0 items/s',
            'é”™è¯¯ç»Ÿè®¡': dict(self.error_counts)
        }
    
    def print_summary(self):
        """æ‰“å°åº¦é‡æ‘˜è¦"""
        summary = self.get_summary()
        
        print("\nğŸ“Š æ‰¹é‡å¤„ç†åº¦é‡æŠ¥å‘Š")
        print("=" * 50)
        
        for key, value in summary.items():
            if key == 'é”™è¯¯ç»Ÿè®¡':
                print(f"{key}:")
                for error_type, count in value.items():
                    print(f"  - {error_type}: {count}")
            else:
                print(f"{key}: {value}")
```

---

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. OpenAI API é™åˆ¶

**é—®é¢˜**: æ‰¹é‡è¯·æ±‚è§¦å‘APIé€Ÿç‡é™åˆ¶

**è§£å†³æ–¹æ¡ˆ**:
```python
import time
from openai import RateLimitError

def handle_rate_limit(func, *args, **kwargs):
    """å¤„ç†APIé€Ÿç‡é™åˆ¶"""
    max_retries = 5
    base_delay = 60  # OpenAIæ¨èç­‰å¾…60ç§’
    
    for attempt in range(max_retries):
        try:
            return func(*args, **kwargs)
        except RateLimitError as e:
            if attempt == max_retries - 1:
                raise e
            
            wait_time = base_delay * (2 ** attempt)
            print(f"âš ï¸ APIé€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)
```

#### 2. å†…å­˜ä¸è¶³

**é—®é¢˜**: å¤„ç†å¤§æ‰¹æ¬¡æ—¶å†…å­˜æº¢å‡º

**è§£å†³æ–¹æ¡ˆ**:
```python
def adaptive_batch_processing(texts: List[str], max_memory_mb: int = 500):
    """è‡ªé€‚åº”æ‰¹æ¬¡å¤§å°å¤„ç†"""
    import psutil
    import sys
    
    # ç›‘æ§å†…å­˜ä½¿ç”¨
    process = psutil.Process()
    
    adaptive_batch_size = 50
    min_batch_size = 10
    max_batch_size = 100
    
    for i in range(0, len(texts), adaptive_batch_size):
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        if memory_mb > max_memory_mb:
            # å‡å°‘æ‰¹æ¬¡å¤§å°
            adaptive_batch_size = max(min_batch_size, adaptive_batch_size // 2)
            print(f"âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜ ({memory_mb:.1f}MB)ï¼Œå‡å°‘æ‰¹æ¬¡å¤§å°åˆ° {adaptive_batch_size}")
        elif memory_mb < max_memory_mb * 0.5:
            # å¢åŠ æ‰¹æ¬¡å¤§å°
            adaptive_batch_size = min(max_batch_size, adaptive_batch_size + 10)
        
        # å¤„ç†å½“å‰æ‰¹æ¬¡
        batch = texts[i:i + adaptive_batch_size]
        try:
            result = embed_texts_batch(batch)
            yield result
        except MemoryError:
            # å¼ºåˆ¶å‡å°‘æ‰¹æ¬¡å¤§å°
            adaptive_batch_size = max(min_batch_size, adaptive_batch_size // 2)
            print(f"âŒ å†…å­˜é”™è¯¯ï¼Œå¼ºåˆ¶å‡å°‘æ‰¹æ¬¡å¤§å°åˆ° {adaptive_batch_size}")
```

#### 3. ç¼“å­˜é—®é¢˜

**é—®é¢˜**: ç¼“å­˜æŸåæˆ–æ€§èƒ½ä¸‹é™

**è§£å†³æ–¹æ¡ˆ**:
```python
def diagnose_cache_health():
    """è¯Šæ–­ç¼“å­˜å¥åº·çŠ¶æ€"""
    from vector.embedding_router import EmbeddingRouter
    import json
    
    router = EmbeddingRouter()
    cache_dir = router.cache_dir
    
    print(f"ğŸ” æ£€æŸ¥ç¼“å­˜ç›®å½•: {cache_dir}")
    
    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶
    cache_files = list(cache_dir.glob("*.json"))
    corrupted_files = []
    
    for cache_file in cache_files:
        try:
            with cache_file.open('r') as f:
                json.load(f)
        except json.JSONDecodeError:
            corrupted_files.append(cache_file)
    
    if corrupted_files:
        print(f"âš ï¸ å‘ç° {len(corrupted_files)} ä¸ªæŸåçš„ç¼“å­˜æ–‡ä»¶")
        
        # æ¸…ç†æŸåçš„æ–‡ä»¶
        for file in corrupted_files:
            file.unlink()
            print(f"ğŸ—‘ï¸ å·²åˆ é™¤æŸåæ–‡ä»¶: {file.name}")
    
    print(f"âœ… ç¼“å­˜å¥åº·æ£€æŸ¥å®Œæˆï¼Œæœ‰æ•ˆæ–‡ä»¶æ•°: {len(cache_files) - len(corrupted_files)}")
```

### æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•

- [ ] **æ‰¹æ¬¡å¤§å°ä¼˜åŒ–**: æ ¹æ®æ–‡æœ¬é•¿åº¦è°ƒæ•´æ‰¹æ¬¡å¤§å°
- [ ] **ç¼“å­˜é¢„çƒ­**: å¯¹å¸¸ç”¨æ–‡æœ¬è¿›è¡Œç¼“å­˜é¢„çƒ­
- [ ] **å†…å­˜ç›‘æ§**: å®æ–½å†…å­˜ä½¿ç”¨ç›‘æ§å’Œè‡ªé€‚åº”è°ƒæ•´
- [ ] **é”™è¯¯é‡è¯•**: å®ç°æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶
- [ ] **è¿›åº¦è·Ÿè¸ª**: æ·»åŠ è¯¦ç»†çš„è¿›åº¦å’Œæ€§èƒ½åº¦é‡
- [ ] **å¹¶å‘é™åˆ¶**: é¿å…è¿‡å¤šå¹¶å‘è¯·æ±‚å¯¼è‡´APIé™åˆ¶
- [ ] **èµ„æºæ¸…ç†**: å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings)
- [Weaviateå®˜æ–¹æ–‡æ¡£](https://weaviate.io/developers/weaviate)

### ç›¸å…³æœ€ä½³å®è·µ
- [å¤§è§„æ¨¡å‘é‡å¤„ç†æœ€ä½³å®è·µ](docs/vector-processing-best-practices.md)
- [ç¼“å­˜ç­–ç•¥æŒ‡å—](docs/caching-strategies.md)
- [æ€§èƒ½ä¼˜åŒ–æ‰‹å†Œ](docs/performance-optimization.md)

### ç¤ºä¾‹ä»£ç 
- [æ‰¹é‡å¤„ç†æ¼”ç¤º](../examples/batch_processing_demo.py)
- [æ€§èƒ½æµ‹è¯•å¥—ä»¶](../tests/test_batch_performance.py)

---

**ğŸ“ è·å–å¸®åŠ©**

å¦‚æœé‡åˆ°é—®é¢˜æˆ–éœ€è¦æŠ€æœ¯æ”¯æŒï¼Œè¯·ï¼š
1. æŸ¥çœ‹[æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)éƒ¨åˆ†
2. æ£€æŸ¥[GitHub Issues](https://github.com/your-repo/issues)
3. è”ç³»æŠ€æœ¯å›¢é˜Ÿ

---

*æ–‡æ¡£ç‰ˆæœ¬: 1.0 | æœ€åæ›´æ–°: 2024-12-19* 